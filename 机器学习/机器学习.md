# 机器学习

## 一、基础概念

通过**特征**形成**专家系统**，利用**算法**对**实例**进行**分类、回归**等操作的方式。



### 常用算法罗列

|    监督学习的用途    |                          |
| :------------------: | :----------------------: |
|     *k*-近邻算法     |         线性回归         |
|    朴素贝叶斯算法    |     局部加权线性回归     |
|      支持向量机      |       *Ridge* 回归       |
|        决策树        | *Lasso* 最小回归系数估计 |
| **无监督学习的用途** |                          |
|       *k*-均值       |       最大期望算法       |
|       *DBSCAN*       |     *Parzen* 窗设计      |

## 二、常用算法

### 1. K-近邻算法

​	存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前*k*个最相似的数据，这就是*k*-近邻算法中*k*的出处，通常*k*是不大于20的整数。最后，选择*k*个最相似数据中出现次数最多的分类，作为新数据的分类。

#### 1.1 简单距离算法构建

```python
def classify0(inX, dataSet, labels, k):
    # 获取数据集大小
    dataSetSize = dataSet.shape[0]
    # np.tile用于沿指定方向复制数组。它的语法是 np.tile(array, reps)，其中 
    # array 是要复制的数组，reps 是指定沿每个轴重复的次数的元组。
    diffMat = np.tile(inX, (dataSetSize,1)) - dataSet
    sqDiffMat = diffMat ** 2
    # 这里实现的就是计算输入点距各数据集中点的距离
    sqDistance = sqDiffMat.sum(axis = 1)
    distances = sqDistance ** 0.5
    # 返回数组中元素排序后的索引
    sortedDistIndicies = distances.argsort()
    classCount = {}
    # 获取前k种可能的情况
    for i in range(k):
        voteIlabel = labels[sortedDistIndicies[i]]
        classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1
    # 根据情况对应次数排序
    sortedClassCount = sorted(classCount.items(),key = operator.itemgetter(1),reverse = True)
    return sortedClassCount[0][0]
```



#### 1.2 从文本中获取数据及分类

##### 1.2.1 matplotlib功能展示

```python
ax.scatter(datingDataMat[:,1], datingDataMat[:,2], 15.0 * np.array(datingLabels), 15.0 * np.array(datingLabels))
```

这行代码使用 matplotlib的 `scatter` 函数绘制散点图。根据提供的参数，它会将 `datingDataMat` 矩阵的第二列作为 x 坐标，第三列作为 y 坐标，并根据 `datingLabels` 数组中的值来确定散点的颜色和大小。

具体解释如下：

- `datingDataMat[:,1]` 表示取 `datingDataMat` 矩阵的所有行的第二列作为 x 坐标。
- `datingDataMat[:,2]` 表示取 `datingDataMat` 矩阵的所有行的第三列作为 y 坐标。
- `15.0 * np.array(datingLabels)` 表示将 `datingLabels` 数组中的每个元素乘以 15.0，生成一个新的数组，用于设置散点的大小。
- `ax.scatter()` 函数的前两个参数是 x 和 y 坐标，第三个参数是每个散点的大小，第四个参数是每个散点的颜色。



##### 1.2.2 标准化

```python
def autoNorm(dataSet):
    # dataSet.min(0)表示从列中取最小值
    minVals = dataSet.min(0)
    maxVals = dataSet.max(0)
    ranges = maxVals - minVals
    normDataSet = np.zeros(np.shape(dataSet))
    m = dataSet.shape[0]
    normDataSet = dataSet - np.tile(minVals, (m,1))
    normDataSet = normDataSet/np.tile(ranges, (m,1))   
    return normDataSet, ranges, minVals
```



##### 1.2.3 整体检测算法

```python
def datingClassTest(filepath,hoRatio,k):
    # 获取数据集
    datingDataMat,datingLabels = file2matrix(filepath)       #load data setfrom file
    normMat, ranges, minVals = autoNorm(datingDataMat)
    # 整个数据集长度
    m = normMat.shape[0]
    # 设定检测数据集范围
    numTestVecs = int(m*hoRatio)
    errorCount = 0.0
    for i in range(numTestVecs):
        # 进行kNN算法检测
        classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],k)
        print ("the classifier came back with: {}, the real answer is: {}".format(classifierResult, datingLabels[i]))
        # 统计错误次数
        if classifierResult != datingLabels[i]: 
            errorCount += 1.0
    print ("the total error rate is: {}" .format(errorCount/float(numTestVecs)))
    print (errorCount)
```



##### 1.2.4 简单的预测算法

```python
def classifyPerson():
    resultList = ['not at all', 'in small doses', 'in large doses']
    percentTats = float(input("请输入用于玩游戏的时间比列："))
    ffMiles = float(input("请输入每年获取的飞行常客里程数："))
    iceCream = float(input("请输入每年冰淇淋的消耗量："))
    datingDataMat,datingLabels = file2matrix('datingTestSet2.txt')
    normMat, ranges, minVals = autoNorm(datingDataMat)
    inArr = np.array([ffMiles, percentTats, iceCream])
    classifierResult = classify0((inArr - minVals) / ranges, normMat, datingLabels, 3)
    print("你可能会喜欢这个人：{}".format(resultList[classifierResult - 1]))
```



#### 1.3 从图像中获取数据

```python
def handwritingClassTest():
    hwLabels = []
    # 读取训练数据文件夹中的文件
    trainingFileList = os.listdir('trainingDigits')           
    m = len(trainingFileList)
    trainingMat = np.zeros((m,1024))
    # 读取为矩阵
    for i in range(m):
        fileNameStr = trainingFileList[i]
        fileStr = fileNameStr.split('.')[0]     
        classNumStr = int(fileStr.split('_')[0])
        hwLabels.append(classNumStr)
        trainingMat[i,:] = img2vector('trainingDigits/' + fileNameStr)
    testFileList = os.listdir('testDigits')        
    errorCount = 0.0
    # 读取测试数据文件夹中的文件
    mTest = len(testFileList)
    errorlist = {}
    for i in range(mTest):
        fileNameStr = testFileList[i]
        fileStr = fileNameStr.split('.')[0]     
        classNumStr = int(fileStr.split('_')[0])
        vectorUnderTest = img2vector('testDigits/' + fileNameStr)
        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3)
        print ("the classifier came back with: {}, the real answer is: {}".format(classifierResult, classNumStr))
        if classifierResult != classNumStr : 
            errorCount += 1.0
            errorlist[fileNameStr] = classifierResult
    print("the total number of errors is: {}".format(errorCount))
    print("the total error rate is: {}".format(errorCount/float(mTest)))
    return errorlist
```



### 2. 决策树

#### 2.1 概念

​	决策树包括**判断模块**、**终止模块**、**分支**，通过判断模块引出分支，可以引向另一个判断模块或者最终到达终止模块，结束决策树的运行。

#### 2.2 流程

1. 收集数据：可以使用任何方法。
2. 准备数据：树构造算法只适用于**标称型数据**，因此数值型数据必须**离散化**。
3. 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
4. 训练算法：构造树的数据结构。
5. 测试算法：使用经验树计算错误率。
6. 使用算法：此步骤可以适用于任何**监督学习算法**，而使用决策树可以更好地理解数据
   的内在含义。

#### 2.3 构造

##### 2.3.1 信息增益

​	在划分数据集之前之后信息发生的变化称为**信息增益**；集合信息的度量方式称为**香农熵**（简称为**熵**）。熵是信息的期望值。

​	如果待分类的事务可能划分在多个分类之中，则符号***x~i~***的信息定义为：
$$
l(x_i)=-log_2p(x_i)
$$

其中$p(x_i)$是选择该分类的概率。

​	由此，我们可以计算所有类别所有可能值包含的信息期望值：

$$
H = -\Sigma{^n_{i=1}}p(x_i)log_2p(x_i)
$$
其中$n$是分类的数目。

##### 2.3.2 基尼不纯度

​	从一个数据集中随机选取子项，度量其被错误分类到其他分组里的概率。

#### 2.4 代码构建决策树

##### 2.4.1 计算香农熵

(原理见[2.3.1](#2.3.1 信息增益)）

```python
# 计算给定数据集的香农熵
def calcShannonEnt(dataSet):
    numEntries = len(dataSet)
    labelCounts = {}
    # 读取数据集中所有的可能性
    for featVec in dataSet: #the the number of unique elements and their occurance
        #获取对应事件 
        currentLabel = featVec[-1]
        # 第一次获取
        if currentLabel not in labelCounts.keys(): 
            labelCounts[currentLabel] = 1
        else:
            labelCounts[currentLabel] += 1
    shannonEnt = 0.0
    for key in labelCounts:
        # 求取事件的可能性
        prob = float(labelCounts[key])/numEntries
        # 求以2为底的对数
        shannonEnt -= prob * log(prob,2) #log base 2
    return shannonEnt
```



##### 2.4.2 根据香农熵划分数据集

```python
# 划分数据集
def splitDataSet(dataSet, axis, value):
    retDataSet = []
    for featVec in dataSet:
        # 去掉数据集中axis列数据为value的值
        if featVec[axis] == value:
            reducedFeatVec = featVec[:axis]     #chop out axis used for splitting
            # 延展列表，相当于去掉了axis列
            reducedFeatVec.extend(featVec[axis+1:])
            retDataSet.append(reducedFeatVec)
    return retDataSet


# 选择最好的数据集划分方式
def chooseBestFeatureToSplit(dataSet):
    #最后一列存放标签
    numFeatures = len(dataSet[0]) - 1      
    # 计算原始香农熵
    baseEntropy = calcShannonEnt(dataSet)
    bestInfoGain = 0.0; bestFeature = -1
    # 遍历所有特征
    for i in range(numFeatures):        
        # 获取当前列的所有数值
        featList = [example[i] for example in dataSet]
        # 建立集合，获取唯一值组成的集合
        uniqueVals = set(featList)       
        newEntropy = 0.0
        # 计算每种划分方式的熵
        for value in uniqueVals:
            subDataSet = splitDataSet(dataSet, i, value)
            prob = len(subDataSet)/float(len(dataSet))
            newEntropy += prob * calcShannonEnt(subDataSet)     
        infoGain = baseEntropy - newEntropy  
        # 判断当前是否为最好的划分方式
        if (infoGain > bestInfoGain):       
            bestInfoGain = infoGain         
            bestFeature = i
    return bestFeature                     
```

**代码解释**：计算完原始的信息熵后，遍历所有的特征，并分别代入划分器当中进行计算，得到最好的划分方式。

**注意事项**：数据必须是一种由列表元素组成的列表，而且所有的列表元素都要具有相同的数据长度；第二个要求是，数据的最后一列或者每个实例的最后一个元素是当前实例的类别标签。数据集一旦满足上述要求，我们就可以在函数的第一行判定当前数据集包含多少特征属性。我们无需限定list中的数据类型，它们既可以是数字也可以是字符串，并不影响实际计算。

##### 2.4.3 递归构建决策树

```python
# 创建决策树
def createTree(dataSet,labels):
    # 提取所有的类别
    classList = [example[-1] for example in dataSet]
    # 如果类别都相等，停止划分
    if classList.count(classList[0]) == len(classList): 
        return classList[0]
    # 仅剩单列可能时返回其出现次数最多的情况
    if len(dataSet[0]) == 1:
        return majorityCnt(classList)
    # 获取最佳特征划分的序列
    bestFeat = chooseBestFeatureToSplit(dataSet)
    # 获取对应特征字符
    bestFeatLabel = labels[bestFeat]
    # 创建树
    myTree = {bestFeatLabel:{}}
    # 去除当前标签
    del(labels[bestFeat])
    featValues = [example[bestFeat] for example in dataSet]
    uniqueVals = set(featValues)
    # 递归所有的特征
    for value in uniqueVals:
        subLabels = labels[:]       
        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)
    return myTree    
```

**代码解释**：递归查看各特征对整体的信息熵的影响，并根据该顺序对数据集进行划分后返回决策树。



##### 2.4.4 使用决策树进行判断

```python
# 使用决策树进行分类
def classify(inputTree,featLabels,testVec):
    # 获取当前判断特征名称
    firstStr = list(inputTree.keys())[0]
    secondDict = inputTree[firstStr]
    # 获取特征的索引
    featIndex = featLabels.index(firstStr)
    key = testVec[featIndex]
    valueOfFeat = secondDict[key]
    # 判断对应的是字典还是值
    if isinstance(valueOfFeat, dict): 
        classLabel = classify(valueOfFeat, featLabels, testVec)
    else: 
        classLabel = valueOfFeat
    return classLabel
```

**代码解释**：利用所创建的决策树和标签列表，通过对列表内特征的判断$testVec$，返回对该对象的分类判断。



##### 2.4.5 存储与读取

```python
# 决策树的存储
def storeTree(inputTree,filename):
    import pickle
    fw = open(filename,'wb')
    pickle.dump(inputTree,fw)
    fw.close()

# 决策树的读取
def grabTree(filename):
    import pickle
    fr = open(filename,'rb')
    return pickle.load(fr)
```

**代码解释**：通过引入$import$模组实现对决策树的读取和存储。



#### 2.5 图形化显示决策树

##### 2.5.1 matplotlib设置

```python
import matplotlib.pyplot as plt

# 指定中文字符集
plt.rcParams['font.sans-serif'] = ['SimHei']  # 指定默认字体为黑体
plt.rcParams['axes.unicode_minus'] = False  # 解决负号'-'显示为方块的问题

# 定义文本框和箭头格式
decisionNode = dict(boxstyle="sawtooth", fc="0.8")
leafNode = dict(boxstyle="round4", fc="0.8")
arrow_args = dict(arrowstyle="<-")
```



##### 2.5.2 创建演示

```python
def createPlot():
    fig = plt.figure(1, facecolor='white')
    fig.clf()
    axprops = dict(xticks=[], yticks=[])
    createPlot.ax1 = plt.subplot(111, frameon=False)   
    plotNode(U'决策节点', (0.5, 0.1), (0.1, 0.5), decisionNode)
    plotNode(U'叶节点', (0.8, 0.1), (0.3, 0.8), leafNode)
    plt.show()
    
def plotNode(nodeTxt, centerPt, parentPt, nodeType):
    createPlot.ax1.annotate(nodeTxt, xy=parentPt,  xycoords='axes fraction',
             xytext=centerPt, textcoords='axes fraction',
             va="center", ha="center", bbox=nodeType, arrowprops=arrow_args )
```

**代码解释**：创建框图及箭头演示。$plotNode(nodeTxt, centerPt, parentPt, nodeType)$代码中，$nodeTxt$中存放要显示的文本，$centerPt$中存放终点坐标，$parentPt$中存放初始点坐标，$nodeType$存放文本框的形态。

效果如下图所示

![演示效果图](F:\Git\Study-Notebook\机器学习\决策树\Figure_1.png)

##### 2.5.3 获取叶节点数量和树的层数

```python
# 获取叶节点的数目
def getNumLeafs(myTree):
    numLeafs = 0
    firstStr = list(myTree.keys())[0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        # 判断对应的值是否是子字典类型
        if type(secondDict[key]).__name__=='dict':
            numLeafs += getNumLeafs(secondDict[key])
        else:   
            numLeafs += 1
    return numLeafs


# 获取树的层数
def getTreeDepth(myTree):
    maxDepth = 0
    firstStr = list(myTree.keys())[0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        # 判断对应的值是否是子字典类型
        if type(secondDict[key]).__name__=='dict':
            thisDepth = 1 + getTreeDepth(secondDict[key])
        else:   
            thisDepth = 1
        if thisDepth > maxDepth: 
            maxDepth = thisDepth
    return maxDepth
```

**代码解释**：递归实现相关数值的获取。



##### 2.5.4 创建树形图的展示

```python
# 创建决策树的展示
def plotTree(myTree, parentPt, nodeTxt):
    # 获取叶节点个数
    numLeafs = getNumLeafs(myTree)
    # 获取层数
    depth = getTreeDepth(myTree)
    firstStr = list(myTree.keys())[0]     
    # 获取初始点坐标
    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)
    plotMidText(cntrPt, parentPt, nodeTxt)
    # 创建第一个框的图像
    plotNode(firstStr, cntrPt, parentPt, decisionNode)
    secondDict = myTree[firstStr]
    plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD
    for key in secondDict.keys():
        # 看子节点是否是一个判断节点
        if type(secondDict[key]).__name__=='dict':
            plotTree(secondDict[key],cntrPt,str(key))        
        else:   
            # 叶节点打印
            plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW
            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)
            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))
    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD
    

def createPlot(inTree):
    fig = plt.figure(1, facecolor='white')
    fig.clf()
    axprops = dict(xticks=[], yticks=[])
    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)   
    # 以下皆为对函数对象的属性创建，目的是对一些数据进行记录，包括上次绘制节点的x、y坐标，叶节点数量和层数等
    plotTree.totalW = float(getNumLeafs(inTree))
    plotTree.totalD = float(getTreeDepth(inTree))
    plotTree.xOff = -0.5/plotTree.totalW
    plotTree.yOff = 1.0
    plotTree(inTree, (0.5,1.0), '')
    plt.show()
```

**代码解释**：利用已有程序对决策树进行绘制。



#### 2.6 示例

```python
import mytree
import treePlot

fr = open('眼镜.txt')
lense = [line.strip().split('\t') for line in fr.readlines()]
labels = ['年龄', '症状', '是否散光', '眼泪频率']
tree = mytree.createTree(lense, labels)
treePlot.createPlot(tree)
```

**代码解释**：读取txt文件后，根据文本形成决策树，然后绘制对应图。

结果如下图所示：

![演示图](F:\Git\Study-Notebook\机器学习\决策树\Figure_2.png)

**注意事项**：对应txt文本本身存在一定保存逻辑，需要对每行数据进行对应，并生成对应的list后才能形成正确的决策树。



### 3. 概率论分类

#### 3.1 概念

基于**贝叶斯决策**理论的分类方法。

**条件概率**是指**事件A**在另外一个**事件B**已经发生条件下的发生概率。此时表示为$p(A|B)$，读作“在B的条件下A的概率”。有以下的公式：
$$
p(A|B)=\frac{p(AB)}{p(B)}
$$

$$
p(c|x) = \frac{p(x|c)p(c)}{p(x)}
$$

