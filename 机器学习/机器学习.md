# 机器学习

## 一、基础概念

通过**特征**形成**专家系统**，利用**算法**对**实例**进行**分类、回归**等操作的方式。



### 常用算法罗列

|    监督学习的用途    |                          |
| :------------------: | :----------------------: |
|     *k*-近邻算法     |         线性回归         |
|    朴素贝叶斯算法    |     局部加权线性回归     |
|      支持向量机      |       *Ridge* 回归       |
|        决策树        | *Lasso* 最小回归系数估计 |
| **无监督学习的用途** |                          |
|       *k*-均值       |       最大期望算法       |
|       *DBSCAN*       |     *Parzen* 窗设计      |

## 二、常用算法

### 1. K-近邻算法

​	存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前*k*个最相似的数据，这就是*k*-近邻算法中*k*的出处，通常*k*是不大于20的整数。最后，选择*k*个最相似数据中出现次数最多的分类，作为新数据的分类。

#### 1.1 简单距离算法构建

```python
def classify0(inX, dataSet, labels, k):
    # 获取数据集大小
    dataSetSize = dataSet.shape[0]
    # np.tile用于沿指定方向复制数组。它的语法是 np.tile(array, reps)，其中 
    # array 是要复制的数组，reps 是指定沿每个轴重复的次数的元组。
    diffMat = np.tile(inX, (dataSetSize,1)) - dataSet
    sqDiffMat = diffMat ** 2
    # 这里实现的就是计算输入点距各数据集中点的距离
    sqDistance = sqDiffMat.sum(axis = 1)
    distances = sqDistance ** 0.5
    # 返回数组中元素排序后的索引
    sortedDistIndicies = distances.argsort()
    classCount = {}
    # 获取前k种可能的情况
    for i in range(k):
        voteIlabel = labels[sortedDistIndicies[i]]
        classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1
    # 根据情况对应次数排序
    sortedClassCount = sorted(classCount.items(),key = operator.itemgetter(1),reverse = True)
    return sortedClassCount[0][0]
```



#### 1.2 从文本中获取数据及分类

##### 1.2.1 matplotlib功能展示

```python
ax.scatter(datingDataMat[:,1], datingDataMat[:,2], 15.0 * np.array(datingLabels), 15.0 * np.array(datingLabels))
```

这行代码使用 matplotlib的 `scatter` 函数绘制散点图。根据提供的参数，它会将 `datingDataMat` 矩阵的第二列作为 x 坐标，第三列作为 y 坐标，并根据 `datingLabels` 数组中的值来确定散点的颜色和大小。

具体解释如下：

- `datingDataMat[:,1]` 表示取 `datingDataMat` 矩阵的所有行的第二列作为 x 坐标。
- `datingDataMat[:,2]` 表示取 `datingDataMat` 矩阵的所有行的第三列作为 y 坐标。
- `15.0 * np.array(datingLabels)` 表示将 `datingLabels` 数组中的每个元素乘以 15.0，生成一个新的数组，用于设置散点的大小。
- `ax.scatter()` 函数的前两个参数是 x 和 y 坐标，第三个参数是每个散点的大小，第四个参数是每个散点的颜色。



##### 1.2.2 标准化

```python
def autoNorm(dataSet):
    # dataSet.min(0)表示从列中取最小值
    minVals = dataSet.min(0)
    maxVals = dataSet.max(0)
    ranges = maxVals - minVals
    normDataSet = np.zeros(np.shape(dataSet))
    m = dataSet.shape[0]
    normDataSet = dataSet - np.tile(minVals, (m,1))
    normDataSet = normDataSet/np.tile(ranges, (m,1))   
    return normDataSet, ranges, minVals
```



##### 1.2.3 整体检测算法

```python
def datingClassTest(filepath,hoRatio,k):
    # 获取数据集
    datingDataMat,datingLabels = file2matrix(filepath)       #load data setfrom file
    normMat, ranges, minVals = autoNorm(datingDataMat)
    # 整个数据集长度
    m = normMat.shape[0]
    # 设定检测数据集范围
    numTestVecs = int(m*hoRatio)
    errorCount = 0.0
    for i in range(numTestVecs):
        # 进行kNN算法检测
        classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],k)
        print ("the classifier came back with: {}, the real answer is: {}".format(classifierResult, datingLabels[i]))
        # 统计错误次数
        if classifierResult != datingLabels[i]: 
            errorCount += 1.0
    print ("the total error rate is: {}" .format(errorCount/float(numTestVecs)))
    print (errorCount)
```



##### 1.2.4 简单的预测算法

```python
def classifyPerson():
    resultList = ['not at all', 'in small doses', 'in large doses']
    percentTats = float(input("请输入用于玩游戏的时间比列："))
    ffMiles = float(input("请输入每年获取的飞行常客里程数："))
    iceCream = float(input("请输入每年冰淇淋的消耗量："))
    datingDataMat,datingLabels = file2matrix('datingTestSet2.txt')
    normMat, ranges, minVals = autoNorm(datingDataMat)
    inArr = np.array([ffMiles, percentTats, iceCream])
    classifierResult = classify0((inArr - minVals) / ranges, normMat, datingLabels, 3)
    print("你可能会喜欢这个人：{}".format(resultList[classifierResult - 1]))
```



#### 1.3 从图像中获取数据

```python
def handwritingClassTest():
    hwLabels = []
    # 读取训练数据文件夹中的文件
    trainingFileList = os.listdir('trainingDigits')           
    m = len(trainingFileList)
    trainingMat = np.zeros((m,1024))
    # 读取为矩阵
    for i in range(m):
        fileNameStr = trainingFileList[i]
        fileStr = fileNameStr.split('.')[0]     
        classNumStr = int(fileStr.split('_')[0])
        hwLabels.append(classNumStr)
        trainingMat[i,:] = img2vector('trainingDigits/' + fileNameStr)
    testFileList = os.listdir('testDigits')        
    errorCount = 0.0
    # 读取测试数据文件夹中的文件
    mTest = len(testFileList)
    errorlist = {}
    for i in range(mTest):
        fileNameStr = testFileList[i]
        fileStr = fileNameStr.split('.')[0]     
        classNumStr = int(fileStr.split('_')[0])
        vectorUnderTest = img2vector('testDigits/' + fileNameStr)
        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3)
        print ("the classifier came back with: {}, the real answer is: {}".format(classifierResult, classNumStr))
        if classifierResult != classNumStr : 
            errorCount += 1.0
            errorlist[fileNameStr] = classifierResult
    print("the total number of errors is: {}".format(errorCount))
    print("the total error rate is: {}".format(errorCount/float(mTest)))
    return errorlist
```



### 2. 决策树

#### 2.1 概念

​	决策树包括**判断模块**、**终止模块**、**分支**，通过判断模块引出分支，可以引向另一个判断模块或者最终到达终止模块，结束决策树的运行。

#### 2.2 流程

1. 收集数据：可以使用任何方法。
2. 准备数据：树构造算法只适用于**标称型数据**，因此数值型数据必须**离散化**。
3. 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
4. 训练算法：构造树的数据结构。
5. 测试算法：使用经验树计算错误率。
6. 使用算法：此步骤可以适用于任何**监督学习算法**，而使用决策树可以更好地理解数据
   的内在含义。

#### 2.3 构造

##### 2.3.1 信息增益

​	在划分数据集之前之后信息发生的变化称为**信息增益**；集合信息的度量方式称为**香农熵**（简称为**熵**）。熵是信息的期望值。

​	如果待分类的事务可能划分在多个分类之中，则符号***x~i~***的信息定义为：
$$
l(x_i)=-log_2p(x_i)
$$

其中$p(x_i)$是选择该分类的概率。

​	由此，我们可以计算所有类别所有可能值包含的信息期望值：

$$
H = -\Sigma{^n_{i=1}}p(x_i)log_2p(x_i)
$$
其中$n$是分类的数目。

##### 2.3.2 基尼不纯度

​	从一个数据集中随机选取子项，度量其被错误分类到其他分组里的概率。

#### 2.4 代码构建决策树

##### 2.4.1 计算香农熵

(原理见[2.3.1](#2.3.1 信息增益)）

```python
# 计算给定数据集的香农熵
def calcShannonEnt(dataSet):
    numEntries = len(dataSet)
    labelCounts = {}
    # 读取数据集中所有的可能性
    for featVec in dataSet: #the the number of unique elements and their occurance
        #获取对应事件 
        currentLabel = featVec[-1]
        # 第一次获取
        if currentLabel not in labelCounts.keys(): 
            labelCounts[currentLabel] = 1
        else:
            labelCounts[currentLabel] += 1
    shannonEnt = 0.0
    for key in labelCounts:
        # 求取事件的可能性
        prob = float(labelCounts[key])/numEntries
        # 求以2为底的对数
        shannonEnt -= prob * log(prob,2) #log base 2
    return shannonEnt
```



##### 2.4.2 根据香农熵划分数据集

```python
# 划分数据集
def splitDataSet(dataSet, axis, value):
    retDataSet = []
    for featVec in dataSet:
        # 去掉数据集中axis列数据为value的值
        if featVec[axis] == value:
            reducedFeatVec = featVec[:axis]     #chop out axis used for splitting
            # 延展列表，相当于去掉了axis列
            reducedFeatVec.extend(featVec[axis+1:])
            retDataSet.append(reducedFeatVec)
    return retDataSet


# 选择最好的数据集划分方式
def chooseBestFeatureToSplit(dataSet):
    #最后一列存放标签
    numFeatures = len(dataSet[0]) - 1      
    # 计算原始香农熵
    baseEntropy = calcShannonEnt(dataSet)
    bestInfoGain = 0.0; bestFeature = -1
    # 遍历所有特征
    for i in range(numFeatures):        
        # 获取当前列的所有数值
        featList = [example[i] for example in dataSet]
        # 建立集合，获取唯一值组成的集合
        uniqueVals = set(featList)       
        newEntropy = 0.0
        # 计算每种划分方式的熵
        for value in uniqueVals:
            subDataSet = splitDataSet(dataSet, i, value)
            prob = len(subDataSet)/float(len(dataSet))
            newEntropy += prob * calcShannonEnt(subDataSet)     
        infoGain = baseEntropy - newEntropy  
        # 判断当前是否为最好的划分方式
        if (infoGain > bestInfoGain):       
            bestInfoGain = infoGain         
            bestFeature = i
    return bestFeature                     
```

**代码解释**：计算完原始的信息熵后，遍历所有的特征，并分别代入划分器当中进行计算，得到最好的划分方式。

**注意事项**：数据必须是一种由列表元素组成的列表，而且所有的列表元素都要具有相同的数据长度；第二个要求是，数据的最后一列或者每个实例的最后一个元素是当前实例的类别标签。数据集一旦满足上述要求，我们就可以在函数的第一行判定当前数据集包含多少特征属性。我们无需限定list中的数据类型，它们既可以是数字也可以是字符串，并不影响实际计算。

##### 2.4.3 递归构建决策树

```python
# 创建决策树
def createTree(dataSet,labels):
    # 提取所有的类别
    classList = [example[-1] for example in dataSet]
    # 如果类别都相等，停止划分
    if classList.count(classList[0]) == len(classList): 
        return classList[0]
    # 仅剩单列可能时返回其出现次数最多的情况
    if len(dataSet[0]) == 1:
        return majorityCnt(classList)
    # 获取最佳特征划分的序列
    bestFeat = chooseBestFeatureToSplit(dataSet)
    # 获取对应特征字符
    bestFeatLabel = labels[bestFeat]
    # 创建树
    myTree = {bestFeatLabel:{}}
    # 去除当前标签
    del(labels[bestFeat])
    featValues = [example[bestFeat] for example in dataSet]
    uniqueVals = set(featValues)
    # 递归所有的特征
    for value in uniqueVals:
        subLabels = labels[:]       
        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)
    return myTree    
```

**代码解释**：递归查看各特征对整体的信息熵的影响，并根据该顺序对数据集进行划分后返回决策树。



#### 2.5 图形化显示决策树

##### 2.5.1 matplotlib设置

```python
import matplotlib.pyplot as plt

# 指定中文字符集
plt.rcParams['font.sans-serif'] = ['SimHei']  # 指定默认字体为黑体
plt.rcParams['axes.unicode_minus'] = False  # 解决负号'-'显示为方块的问题

# 定义文本框和箭头格式
decisionNode = dict(boxstyle="sawtooth", fc="0.8")
leafNode = dict(boxstyle="round4", fc="0.8")
arrow_args = dict(arrowstyle="<-")
```

