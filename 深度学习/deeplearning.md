# 深度学习

本文基于对《基于python深度学习实战》一书的学习，以下内容大部分出自该书。

## 一、基础概念

### 1. 机器学习简述

#### 1.1 概率建模



#### 1.2 早期神经网络



#### 1.3 核方法



#### 1.4 决策树、随机森林与梯度提升机



### 2. 神经网络的数学基础

#### 2.1 组成

分类问题中某个类别被称作**类**（class）；数据点叫做**样本**（sample）；某个样本对应的类叫做**标签**（label）。

模型通过对**训练集**的学习后，对**测试集**进行测试，验证模型的准确性和性能。

神经网络的核心组件是**层**（layer），这是一种数据处理模块，通过对输入数据进行过滤筛选，提取其中更需要的内容。通过将层进行链接，从而实现渐进式的**数据蒸馏**。深度学习的模型就包含一系列逐步精细的层。

要想训练网络，我们还需要选择**编译**步骤的三个参数。

- **损失函数**（loss function）：网络如何衡量在训练数据上的性能，即网络如何朝着正确的方向前进
- **优化器**（optimizer）：基于训练数据和损失函数来更新网络的机制
- **指标**（metric）：精度等（在训练和测试过程中需要监控的参数）



#### 2.2 代码演示

```python
from keras.datasets import mnist
from keras import models
from keras import layers
from keras.utils import to_categorical
# 准备训练集和测试集
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
# 引入神经网络模型
network = models.Sequential()
# 添加2个密集连接（也称全连接，Dense）层
network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))
# 10路softmax层,将会返回一个由10个概率值(总和为1)组成的数组
network.add(layers.Dense(10, activation='softmax'))
# 实现编译功能
network.compile(optimizer='rmsprop',
                loss='categorical_crossentropy',
                metrics=['accuracy'])
# 数据集预处理,将(60000,28,28)转为(60000, 28 * 28),并且归一化
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype('float32') / 255
test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype('float32') / 255
# 对标签进行分类编码
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)
# 通过训练数据集进行模型拟合(fit)
network.fit(train_images, train_labels, epochs=5, batch_size=128)
# 利用测试集进行性能验证
test_loss, test_acc = network.evaluate(test_images, test_labels)
print('test_acc:', test_acc)
```

**代码解释**：通过引入神经网络模型，利用训练集对模型进行训练，再对测试集进行测试，通过精度反馈模型效果。

**注意事项**：本示例中训练集训练后精度为98%，但测试后精度为97.8%左右，这种训练精度和测试精度的差距被称作**过拟合**。



#### 2.3 神经网络的数据表示

数据存储在多维Numpy数组中，叫做**张量**（tensor）,这是机器学习系统中基本的数据结构。张量的**维度**（dimension）通常被称作**轴**（axis）。

##### 2.3.1 标量（0D张量）

仅包含一个数字的张量叫作**标量**（scalar，也叫标量张量、零维张量、0D 张量）。

在numpy中，一个 float32 或 float64 的数字就是一个标量张量（或标量数组）。可以用 **ndim** 属性查看一个张量的轴的个数，张量轴的个数也叫做**阶**（rank）。

```python
import numpy as np
x = np.array(12)
print(x.ndim)
```

运行后结果为0

##### 2.3.2 向量（1D张量）

数字组成的数组叫做**向量**（vector，也叫作一维张量，1D张量），只有一个轴。

一个向量中包含n个元素，可作**nD向量**，注意不要同nD张量搞混。

##### 2.3.3 矩阵（2D张量）

向量组成的数组称作**矩阵**（matrix，二维张量，2D张量）。矩阵有两个轴（通常被称作**行**和**列**）。

##### 2.3.4 多阶张量

多个矩阵的组合可以形成多阶的张量

##### 2.3.5 关键属性

- **轴的个数（阶）**	可以用np中的ndim属性查看。
- **形状**	一个整数元组，表示张量沿每个轴的维度大小（个数）。
- **数据类型**  	张量所包含数据的类型，一般是uint8、float32、float64等。



#### 2.4 张量运算

##### 2.4.1 逐元素运算

包括逐元素的**加法**和**relu算法**，在numpy中可以用代码快速实现

```python
import numpy as np

z = x + y
#relu算法是一种滤值方法，通过比较元素同0的大小，保留大于0的部分
z = np.maximum(z, 0.)
```



##### 2.4.2 广播

如果将两个形状不同的张量相加时，没有歧义的话，较小的张量会被**广播**（broadcast），以匹配较大张量的形状。

广播包含以下两步:

1. 向较小的张量添加轴（叫作广播轴），使其 ndim 与较大的张量相同。
2. 将较小的张量沿着新轴重复，使其形状与较大的张量相同。



##### 2.4.3 张量点积

点积运算，也叫**张量积**，是最常见也最有用的张量运算。与逐元素的运算不同，它将输入张量的元素合并在一起。

![张量点积](F:\Git\Study-Notebook\深度学习\material\2.1.png)



##### 2.4.4 张量变形

张量变形是指改变张量的行和列，以得到想要的形状。变形后的张量的元素总个数与初始张量相同。

经常遇到的一种特殊的张量变形是转置（transposition）。对矩阵做转置是指将行和列互换，使 x[i, :] 变为 x[:, i]。



#### 2.5 基于梯度的优化

##### 2.5.1 概念

下面的式子展示了一种简单的神经层运算方法：

$$output = relu(dot(W, input) + b)$$

其中，W 和 b 都是张量，均为该层的属性。它们被称为该层的**权重**（weight，或者**可训练参数**，trainable parameter），这些权重包含网络从观察

训练数据中学到的信息。

一开始，这些权重矩阵取较小的随机值，这一步叫作**随机初始化**（random initialization）。当然，W 和 b 都是随机的，relu(dot(W, input) + b) 肯定不会得到任何有用的表示。虽然得到的表示是没有意义的，但这是一个起点。下一步则是根据反馈信号逐渐调节这些权重。这个逐渐调节的过程叫作**训练**，也就是机器学习中的学习。上述过程发生在一个**训练循环**（training loop）内，其具体过程如下。必要时一直重复这些步骤。

1.  抽取训练样本 x 和对应目标 y 组成的数据批量。
2.  在 x 上运行网络［这一步叫作**前向传播**（forward pass）］，得到预测值 y_pred。
3.  计算网络在这批数据上的损失，用于衡量 y_pred 和 y 之间的距离。
4.  更新网络的所有权重，使网络在这批数据上的损失略微下降。

最终得到的网络在训练数据上的损失非常小，即预测值 y_pred 和预期目标 y 之间的距离非常小。

其中，更新网络权重的步骤是较为困难的，很多时候不知道如何变动各项权重，盲目改变后重新计算是非常低效的，为此，一种更好的方法是利用网络中所有运算都是**可微**（differentiable）的这一事实，计算损失相对于网络系数的**梯度**（gradient），然后向梯度的反方向改变系数，从而使损失降低。

梯度是张量运算的导数。它是导数这一概念向多元函数导数的推广。多元函数是以张量作为输入的函数。

##### 2.5.2 随机梯度下降

给定一个可微函数，理论上可以用解析法找到它的最小值：函数的最小值是导数为 0 的点，因此你只需找到所有导数为 0 的点，然后计算函数在其中哪个点具有最小值。

由于处理的是一个可微函数，可以计算出它的梯度，从而有效地实现第四步。沿着梯度的反方向更新权重，损失每次都会变小一点。

1.  抽取训练样本 x 和对应目标 y 组成的数据批量。
2.  在 x 上运行网络，得到预测值 y_pred。
3.  计算网络在这批数据上的损失，用于衡量 y_pred 和 y 之间的距离。
4.  计算损失相对于网络参数的梯度［一次**反向传播**（backward pass）］。
5.  将参数沿着梯度的反方向移动一点，比如 W -= step * gradient，从而使这批数据上的损失减小一点。

这种方法叫作**小批量随机梯度下降**（mini-batch stochastic gradient descent，又称为**小批量 SGD**）

每次迭代时只抽取一个样本和目标，而不是抽取一批数据，这叫作**真 SGD**。

每一次迭代都在所有数据上运行，这叫作**批量 SGD**。

此外，SGD 还有多种变体，其区别在于计算下一次权重更新时还要考虑上一次权重更新，而不是仅仅考虑当前梯度值，比如带动量的 **SGD**、**Adagrad**、**RMSProp** 等变体。这些变体被称为**优化方法**（optimization method）或**优化器**（optimizer）。

动量解决了 SGD 的两个问题：收敛速度和局部极小点。动量方法的实现过程是每一步都移动小球，不仅要考虑当前的斜率值（当前的加速度），还要考虑当前的速度（来自于之前的加速度）。这在实践中的是指，更新参数 w 不仅要考虑当前的梯度值，还要考虑上一次的参数更新。

##### 2.5.3 反向传播算法

根据微积分的知识，这种函数链可以利用下面这个恒等式进行求导，它称为**链式法则**（chain rule）：$(f(g(x)))' = f'(g(x)) * g'(x)$​。将链式法则应用于神经网络梯度值的计算，得到的算法叫作**反向传播**（backpropagation，有时也叫反式微分，reverse-mode differentiation）。反向传播从最终损失值开始，从最顶层反向作用至最底层，利用链式法则计算每个参数对损失值的贡献大小。

### 3. 神经网络入门



